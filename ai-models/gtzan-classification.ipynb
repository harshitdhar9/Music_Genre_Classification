{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1032238,"sourceType":"datasetVersion","datasetId":568973}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gtzan classification using the melspectogram images","metadata":{"_uuid":"d0b95b32-6f0f-4927-910b-f4904b90228e","_cell_guid":"7b543960-865c-4422-9e6c-1d41ac85f355","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"#Importing the libraries \n\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport shutil\n\nimport librosa\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nprint(tf.__version__)","metadata":{"_uuid":"437c6745-ec5f-43de-af38-360cb6d704ea","_cell_guid":"5324d49e-4fbc-4ea1-a3e4-4c1a9caabfb5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T20:41:10.986051Z","iopub.execute_input":"2025-04-17T20:41:10.986402Z","iopub.status.idle":"2025-04-17T20:41:10.993011Z","shell.execute_reply.started":"2025-04-17T20:41:10.986377Z","shell.execute_reply":"2025-04-17T20:41:10.992056Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# How melspectogram is generated using librosa \n> (Important features here i learnt are : n_mels,short term forier transform, vmin and vmax range to display the most important sounds)","metadata":{"_uuid":"9482ddc9-c042-4d98-aa41-9a7ca81ddfb9","_cell_guid":"4b4f833d-7a25-45c2-bbae-e2507fadb75c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"#Librosa functions to see how the melspecs are generated \n\naudio_file = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/classical/classical.00007.wav'\n\ny, sr = librosa.load(audio_file)\nprint(y.shape)   #number of frames\nprint(sr)    #samples per second\nD = librosa.stft(y)\nprint(D.shape)\nS = librosa.feature.melspectrogram(S=D, sr=sr, n_mels=256, fmax=8000)\n\n\"\"\"\nn_mels = 256, you're dividing the spectrum (from 0 Hz to fmax) \ninto 256 non-linear frequency bins.\nmore resolution at low frequencies and vica versa (like ears)\n\"\"\"\n\nprint(S.shape)\nS_db = librosa.power_to_db(np.abs(S), ref=np.max) #to db for visualisation\nprint(S_db.shape)\nplt.figure(figsize=(6, 4))\nlibrosa.display.specshow(S_db, x_axis='time', y_axis='mel', sr=sr,vmin=-25,vmax=0)\n#O db being the loudest sound and others being relative to that so we will take the values that are most relavant [ref=np.max],np.abs used to handle imaginary values\n\nplt.colorbar(format='%+2.0f dB')\nplt.title('Mel Spectrogram')\nplt.show()","metadata":{"_uuid":"8bacf3d0-9fc8-4eee-a3b2-124c78a20e80","_cell_guid":"83bcb5c4-3e02-4704-a5c0-7f29b3299592","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T20:16:50.970618Z","iopub.execute_input":"2025-04-17T20:16:50.971103Z","iopub.status.idle":"2025-04-17T20:16:51.597347Z","shell.execute_reply.started":"2025-04-17T20:16:50.971081Z","shell.execute_reply":"2025-04-17T20:16:51.596368Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Viewing the Melspec images of various generes\n> Tensorflow libraries used here\n> * tf.io\n> * tf.image","metadata":{"_uuid":"1e6a667a-4413-4ef6-b043-4572c6777ce0","_cell_guid":"db5f4fba-9ac0-47d1-964b-1b2d6ff7e87d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"#folder containing subfolders of melspec images\nfolder_path='/kaggle/input/gtzan-dataset-music-genre-classification/Data/images_original'\n\n\"\"\"\nMelspec spatial features can be visualised through cnn's each genre has different pattern \nof melspec which the model captures and thus can differentiate between music types\n\"\"\"\n\ntotal_genres=10\n\n#file path for testing\ndictionary={\n    \"Hiphop\":'/kaggle/input/gtzan-dataset-music-genre-classification/Data/images_original/hiphop/hiphop00001.png',\n    \"Metal\":'/kaggle/input/gtzan-dataset-music-genre-classification/Data/images_original/metal/metal00009.png',\n    \"Classical\":'/kaggle/input/gtzan-dataset-music-genre-classification/Data/images_original/classical/classical00007.png',\n    \"Rock\":'/kaggle/input/gtzan-dataset-music-genre-classification/Data/images_original/rock/rock00010.png',\n    \"Blues\":'/kaggle/input/gtzan-dataset-music-genre-classification/Data/images_original/blues/blues00048.png'\n    \n}\n\ndef printing_melspecs(file):\n    raw=tf.io.read_file(file)\n    image=tf.io.decode_png(raw,channels=3)\n    \n    print(image.shape)\n    #print(image.dtype)\n    #print(image)\n\n    image=tf.image.convert_image_dtype(image,tf.float32)\n    image=tf.image.adjust_brightness(image,delta=0.2)\n\n    #This not needed as we converted image dtype to adjust brightness\n    #image_arr=image.numpy().astype(\"float32\") / 255.0  \n    \n    image_arr=image.numpy() #(to numpy for visualisation)\n    \n    plt.figure(figsize=(6,4))\n    plt.imshow(image_arr,cmap='cool',interpolation='bicubic')\n\n    plt.axis('off')\n    plt.title('Melspectogram')\n    plt.show()\n\nfor k,v in dictionary.items():\n    print(k)\n    printing_melspecs(v)","metadata":{"_uuid":"103166be-80e2-4d0e-bc15-9c515e3f64d1","_cell_guid":"6afc135b-186f-4597-ba62-b22fb4af2e03","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T20:16:51.598680Z","iopub.execute_input":"2025-04-17T20:16:51.599018Z","iopub.status.idle":"2025-04-17T20:16:52.496642Z","shell.execute_reply.started":"2025-04-17T20:16:51.598989Z","shell.execute_reply":"2025-04-17T20:16:52.495641Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":">Notice the difference between melspectograms of different genres,\nX axis is time and Y axis is frequency , the colored part represent pitch or amplitude , \nit used Mel scale , similar to how humans perceive audio,(change of frequencies over time)","metadata":{"_uuid":"fd215d6d-a6ad-4b23-991f-33a5d80b0717","_cell_guid":"be5a8641-57e6-4c24-bd59-7bac2736c659","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Splitting the images from folder into train,test,val sets","metadata":{"_uuid":"a351bb08-a50c-4453-bc67-11a26bb3c20f","_cell_guid":"5173edfd-0d14-4c26-b603-ab5332bb4f8b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"base_dir = '/kaggle/working/'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'val')\ntest_dir = os.path.join(base_dir, 'test')\n\nos.makedirs(train_dir, exist_ok=True)\nos.makedirs(val_dir, exist_ok=True)\nos.makedirs(test_dir, exist_ok=True)\n\nsplit_ratio = {'train': 0.7, 'val': 0.15, 'test': 0.15}","metadata":{"_uuid":"be8eff29-8313-4655-a188-c534502a264f","_cell_guid":"6a887eb0-f1de-47f7-93ca-aa328c5f0847","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T20:38:21.599840Z","iopub.execute_input":"2025-04-17T20:38:21.604410Z","iopub.status.idle":"2025-04-17T20:38:21.621449Z","shell.execute_reply.started":"2025-04-17T20:38:21.604313Z","shell.execute_reply":"2025-04-17T20:38:21.620125Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for genre in os.listdir(folder_path):\n    genre_folder = os.path.join(folder_path, genre)\n    if os.path.isdir(genre_folder):\n      \n        os.makedirs(os.path.join(train_dir, genre), exist_ok=True)\n        os.makedirs(os.path.join(val_dir, genre), exist_ok=True)\n        os.makedirs(os.path.join(test_dir, genre), exist_ok=True)\n\n        images = os.listdir(genre_folder)\n        random.shuffle(images)\n\n        total_images = len(images)\n        train_size = int(total_images * split_ratio['train'])\n        val_size = int(total_images * split_ratio['val'])\n\n        train_images = images[:train_size]\n        val_images = images[train_size:train_size + val_size]\n        test_images = images[train_size + val_size:]\n\n        for image in train_images:\n            shutil.copy(os.path.join(genre_folder, image), os.path.join(train_dir, genre, image))\n        for image in val_images:\n            shutil.copy(os.path.join(genre_folder, image), os.path.join(val_dir, genre, image))\n        for image in test_images:\n            shutil.copy(os.path.join(genre_folder, image), os.path.join(test_dir, genre, image))\n\nprint(\"Dataset split completed!\")","metadata":{"_uuid":"1986fb75-8023-4d7b-909f-743a88c754a0","_cell_guid":"45954d37-52f2-44db-aad0-c0bad38c106b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-17T20:47:45.319982Z","iopub.execute_input":"2025-04-17T20:47:45.320344Z","iopub.status.idle":"2025-04-17T20:47:46.759140Z","shell.execute_reply.started":"2025-04-17T20:47:45.320322Z","shell.execute_reply":"2025-04-17T20:47:46.758377Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Building a Sequential Keras Model of conv layers , dropout, batchnormalisation , etc.\n> Padding/cropping/resizing due to different image dimensions , conv2d expects a consistent input tensor dim.","metadata":{"_uuid":"b5406be8-d27b-4021-be4d-d7018714541a","_cell_guid":"fc303d16-7568-4c12-909b-acf6eee47a08","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"","metadata":{"_uuid":"11c9c306-1a80-4f31-928f-9e0dd3b6defe","_cell_guid":"7a128fa0-413d-4ad8-925e-49452a093242","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}