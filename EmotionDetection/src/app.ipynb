{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "from capture import CameraCapture\n",
    "from preprocessing import EmotionPreprocessor\n",
    "from emotiondetection import EmotionDetector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MoodBasedMusicSystem:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the mood-based music system.\"\"\"\n",
    "        self.camera = CameraCapture()\n",
    "        self.preprocessor = EmotionPreprocessor()\n",
    "        self.emotion_detector = EmotionDetector()\n",
    "    \n",
    "    def capture_and_detect_mood(self, display_countdown=True):\n",
    "        \"\"\"Capture image from camera and detect mood.\"\"\"\n",
    "        try:\n",
    "            # Capture image\n",
    "            if display_countdown:\n",
    "                frame = self.camera.display_with_feedback(seconds=5)\n",
    "            else:\n",
    "                frame = self.camera.capture_frame()\n",
    "            \n",
    "            # Save captured frame for reference\n",
    "            frame_path = self.camera.save_frame(frame)\n",
    "            print(f\"Captured frame saved to: {frame_path}\")\n",
    "            \n",
    "            # Preprocess the image\n",
    "            preprocessed_face = self.preprocessor.preprocess_image(frame)\n",
    "            \n",
    "            if preprocessed_face is None:\n",
    "                print(\"No face detected in the captured image\")\n",
    "                return {\"error\": \"No face detected\"}, None\n",
    "            \n",
    "            # Detect emotion\n",
    "            emotion_result = self.emotion_detector.predict_emotion(preprocessed_face)\n",
    "            \n",
    "            # Map to mood\n",
    "            mood = self.emotion_detector.map_emotion_to_mood(emotion_result)\n",
    "            \n",
    "            print(f\"Detected emotion: {emotion_result['emotion']} (Confidence: {emotion_result['confidence']:.2f})\")\n",
    "            print(f\"Mapped mood for music generation: {mood}\")\n",
    "            \n",
    "            return emotion_result, mood\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in mood detection: {e}\")\n",
    "            return {\"error\": str(e)}, \"neutral\"\n",
    "    \n",
    "    def capture_and_detect_mood_from_image(self, image_path):\n",
    "        \"\"\"Detect mood from a given image.\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            frame = cv2.imread(image_path)\n",
    "            \n",
    "            # Preprocess the image\n",
    "            preprocessed_face = self.preprocessor.preprocess_image(frame)\n",
    "            \n",
    "            if preprocessed_face is None:\n",
    "                print(\"No face detected in the captured image\")\n",
    "                return {\"error\": \"No face detected\"}, None\n",
    "            \n",
    "            # Detect emotion\n",
    "            emotion_result = self.emotion_detector.predict_emotion(preprocessed_face)\n",
    "            \n",
    "            # Map to mood\n",
    "            mood = self.emotion_detector.map_emotion_to_mood(emotion_result)\n",
    "            \n",
    "            print(f\"Detected emotion: {emotion_result['emotion']} (Confidence: {emotion_result['confidence']:.2f})\")\n",
    "            print(f\"Mapped mood for music generation: {mood}\")\n",
    "            \n",
    "            return emotion_result, mood\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in mood detection: {e}\")\n",
    "            return {\"error\": str(e)}, \"neutral\"\n",
    "        \n",
    "    def send_mood_to_music_model(self, mood):\n",
    "        \"\"\"Send detected mood to the music generation model.\"\"\"\n",
    "        # This would be an API call or a function call to your music generation model\n",
    "        print(f\"Sending mood '{mood}' to music generation model\")\n",
    "        \n",
    "        # Placeholder for the actual implementation\n",
    "        # Example: music_model.generate(mood=mood)\n",
    "        \n",
    "        return {\"status\": \"success\", \"mood_sent\": mood}\n",
    "    \n",
    "    def release_resources(self):\n",
    "        \"\"\"Release camera and other resources.\"\"\"\n",
    "        self.camera.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion detection model loaded successfully\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021C50B75B40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021C50B75B40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "Detected emotion: surprise (Confidence: 0.80)\n",
      "Mapped mood for music generation: energetic\n",
      "Sending mood 'energetic' to music generation model\n",
      "Result: {'status': 'success', 'mood_sent': 'energetic'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    x = input(\"Enter 1 for real time mood detection and 2 for image mood detection\")\n",
    "    mood_system = MoodBasedMusicSystem()\n",
    "    if(x == 1):\n",
    "        try:\n",
    "            # Detect mood from camera\n",
    "            emotion_data, mood = mood_system.capture_and_detect_mood()\n",
    "                \n",
    "            if \"error\" not in emotion_data:\n",
    "                # Send mood to music generation model\n",
    "                result = mood_system.send_mood_to_music_model(mood)\n",
    "                print(\"Result:\", result)\n",
    "                \n",
    "        finally:\n",
    "            # Release resources\n",
    "            mood_system.release_resources()\n",
    "    else : \n",
    "        try:\n",
    "            # Detect mood from image\n",
    "            emotion_data, mood = mood_system.capture_and_detect_mood_from_image(\"E:\\\\Codes\\\\EmotionDetection\\\\Test_images\\\\test_image_4.jpg\")\n",
    "                \n",
    "            if \"error\" not in emotion_data:\n",
    "                # Send mood to music generation model\n",
    "                result = mood_system.send_mood_to_music_model(mood)\n",
    "                print(\"Result:\", result)\n",
    "                \n",
    "        finally:\n",
    "            # Release resources\n",
    "            mood_system.release_resources()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
